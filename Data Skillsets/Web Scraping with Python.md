```ad-resources
https://github.com/REMitchell/python-scraping
```
## Web Scraping Basic Concept
### Definition
> In theory, **web scraping** is the practice of *gathering data from web* through *any means other than a program interacting with an API*.
- If the sources have API -> maybe better to use API. 
	- However, sometimes API is not designed for our specific tasks + request volume + rate limits + not wanted type and format of data
- Commonly, web scraping using **automated program** to:
	1. queries a web server
	2. requests data
	3. parses that data to extract needed info
### Advantages:
Web scraper can:
- access and extract *large amounts data* (parallel thousands and millions web pages) with *very high speed*
- provide *much deeper* and *more details* information than Google search
- *handle specific things* that API can not do
## Web Scraping Workflow
[[Web Scraping Workflow]]
## Example in Book 
- Crawling Across the Internet **(page 43 & 46)**
- Crawling Sites Through Search **(page 59 -> 61)**
- Crawling Sites Through Links **(page 62, 63)**
- Crawling Multiple Page Types **(page 64, 65)**
- Storing scraped data in CSV files **(page 87)**
- Scraping title and content of Wikipedia's pages and storing them to a MySQL Database using *BeautifulSoup* and *PyMySQL* libraries **(page 96)**
- "Six Degrees" in MySQL **(page 100)**
## Building Scraper
### A simple web Scraper
- To scraping, we need to move away from the browser level (with all the beautiful UI built by HTML, CSS and JS) to work in *the level of network connection*
- page 3 | simple example how one machine 1 (Bob's computer) talks to other machine 2 (Alice's server) on the internet.
- Better to thinking these addresses which we scrape (e.g. http://pythonscraping.com/pages/page1.html) as "file" (e.g. page1.html) but not as "page" in a browser because a modern web page have many resources files associated with them -> "file" is more specific -> *thinking in "files" make it easier to get what we need.*
> Good practices: Using generic functions (e.g. `getTitle()` or `getSiteHTML()`) to get HTML and handle errors and exceptions.
#### First tool: [[Urllib library]]
- `urllib` is a built-in standard Python library which contains functions for requesting data across the web , handling cookies, and even changing metadata.
- `urllib` [documentation](https://docs.python.org/3.9/library/urllib.html)
#### Second tool: [[BeautifulSoup library]]
- ![[BeautifulSoup library#A simple sample| A simple BS sample]]
- **Attention!** Using Virtual Environments to manage library. Why? and How?
 #### Handling exceptions and connection error
- Problems or errors (connection, data format...) often happen when we leave the computer alone to do scrapping -> **problems need to be anticipated and well managed.**
- **Problems with `urlopen` line:**
	- ![[Pasted image 20220204084215.png|700]]
	- Situation 1: If page is not found on the server (e.g. HTTP error as "404 Page Not Found") -> using exception `HTTPError` from `urllib.error`
	- Situation 2: If server is not found (e.g. server is down) -> using exception `URLError` from `urllib.error`
	- *Solution:*
		- ![[Pasted image 20220204091146.png|700]]
- **Problems with HTML content:**
	 - Situation: Access a tag that does not exist in `BeautifulSoup` object will return a `None` object and a `AttributeError` -> using exception `AttributeError` and exclude the `None` object case.
	 - *Solution:*
		 - ![[Pasted image 20220204092116.png|500]]
### Advanced HTML parsing
```ad-important
Main idea: **Chip out the unneeded content from the HTML until you get the content you need.**
```
```ad-important
**Parsing code properties:**
- Robust: still work even there are significant change in the HTML code.
- Readability: easy to read -> understand -> debug -> reuse
```
- page 16 | **Advanced parsing may and should be the last option**, before try it you should try to:
	1. Look for better-formatted option (e.g. "Print This Page" link or mobile version)
	2. Look for the info hidden in a JS file
	3. The info may be available in the URL of the page itself
	4. Other websites or sources have the same data but easier access?
#### Using BeautifulSoup library
- 2 most useful tools for [[HTML]] parsing in [[BeautifulSoup library]]:
	- `soup.find()` and `soup.find_all()` functions
	- [[BeautifulSoup library#Navigating Trees|Navigating Trees]]
#### Using Regular Expressions (regex)
- *Regex* used to identify *regular strings* (any string that can be generated by a series of linear rules), which means:
	1. strings which follow the regex rules -> `True`
	2. strings which don't follow the regex rules -> `False`
- More details for [[Regular Expressions|regex]]
- page 26 | example of using regex to identify a quite complex regular strings
- Writing regex's best practices:
	1. break the regular strings to a smaller manageable parts
	2. write regex for each part
	3. combine them.  
- page 29 | example of using regex to identify common email addresses:
	- `[A-Za-z0-9\._+]+@[A-Za-z]+\.(com|org|edu|net)`
#### Regex with BeautifulSoup
- In **Beautiful Soup library**, most functions that take a string arguments (e.g. `find(id="aTagIdHere")`) will also take in a regex as well.
- Example:
	- ![[Pasted image 20220207111403.png|700]]
#### Lambda expression
- page 31 | BeautifulSoup allows to pass certain types of functions (often [[Functions in Python#Lambda function or Lambda expression|lambda functions]]) as parameters in to the `soup.find_all()` function.
	- Condition of these parameter functions:
		1. must take a `tag` object as an argument
		2. return a boolean. If `True` -> return the `tag`, if `False` -> not return.
	- Example:
		- e.g.1. ![[Pasted image 20220207114613.png|900]]
		- e.g.2. ![[Pasted image 20220207115351.png|700]]
	- Thinking about combine lambda function with regex!
### Writing Web Crawlers
```ad-info
**Web crawlers** is a piece of code (or software) that crawl across the web by using an element of recursion: examine a page for a URL, retrieve needed info on that page, examine another page for another URL, retrieve needed info on the other page and ad infinitum.
```
```ad-important
**Steps to Process:**
- Know exactly what we want to scrap -> See them in html file
- *Find the common pattern which separates the thing we need from other things*
- Write loop with that pattern
- Test & Run
```
- page 35 | Example: Get all the link from a wiki page
	- Code:
	```python
	from urllib.request import urlopen  
	from bs4 import BeautifulSoup  
	import re  
	def get_links(article_url):
		html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')  
		bs = BeautifulSoup(html, 'html.parser')  
		return bs.find('div', {'id':'bodyContent'}).find_all(  
			'a', href=re.compile('^(/wiki/)((?!:).)*$')): # return all tag 'a' (link) which have attr 'href' wiki link pattern
			
	links = get_link('/wiki/Kevin_Bacon')
	for link in links:
		if 'href' in link.attrs:
			print(link.attrs['href']) # print the href link
  ```
#### Crawling an Entire Site
- How? 
	- Process:
		1. Begin from the top-level page 
		2. Get all link to a `set`
		3. Choose (randomly) to go to a link from the list 4.
		4. Get all new link (non-duplicate) from chosen link and add them to the `set`
		5. Recurring  the process until the list is exhausted.
		- Here, we uses [[Built-in Types in Python#set type|`set` type]] as a `set` doesn't have specific order and only unique elements will be stored.
	- e.g.  for Wikipedia
		- ![[Pasted image 20220221094138.png|750]]
- Advantages of crawling an entire website:
	- Generating a site map (for design reason?)
	- Collecting data while crawling across an entire site (page 41)
		1. Identify the data we want by looking at a few pages -> find patterns!
		2. Access page
		3. Get and store wanted data (remember to handle exception)
		4. Continue crawling in all not-yet-crawled page (loop step 2 -> 3 -> 4 until no uncrawled page left)
#### Crawling Across the Internet
```ad-caution
Always ask about which data you need to collect and where to find them (do we need to crawl all over the place?)
```
- Write down algorithm diagram before coding (best practice)
	- e.g. ![[Pasted image 20220221110309.png|700]]
- Write code down (prefer using functions to encapsulate code)
	- e.g. page 43 and page 46
### Web Crawling Models
- Clean and scalable scraping code can be a challenge. (Why? messy and diverse templates and layout) => **scrapping code should be flexible, easy to refactor** to adapt new websites or formats.
```ad-important
- Identify clearly what you need/want to scrap
- Think thoroughly about **maintainability** and **robustness** of your web scrapper
	- Normalize scrapping data (using patterns) and scrapping process (using functions)
	- Anticipate what can happen (what if new source needs to scrap -> no/not much future maintaining need to be performed)
- Adapt the data model (how to design and store data) to maximize **maintainability** and **robustness**
```
#### Planning and Defining Objects
1. First question: **"What do I need?"**
	- This info is already exist in our DB?
	- This info is indispensable for the project or just "nice to have"?
	- If the info *might* help in future, is it hard to scrap?
	- If this data is necessary, we need to know more about the nature of data (page 51):
		- How large?
		- Sparse or dense?
		- Need to retrieve regularly to process the analysis?
		- How frequently add and remove data or attributes?
2. Second question: **"How design data model (design and store data) to maximize maintainability and robustness?"**
	- Think thoroughly -> anticipate what can happen (if new source need to scrap in the future)
	- Adapt the data model (how to design and store data) as follow
	- Normalize the scrapping process.
- Example of scrapping several sites' title and body using common website structure but still need to input link by hand (page 55->57):
	- Using [`requests`](https://docs.python-requests.org/en/latest/) library instead of `urllib`
	- Using [CSS selector](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors) instead of BS built-in searching functions such as `find()` or `find_all()`
#### Structuring Crawlers
```ad-info
3 basic web crawler structure (which automate gather links and discover data):
- Crawling Sites Through Search
- Crawling Sites Through Links
- Crawling Multiple Page Types
```
##### Crawling Sites Through Search
- How? The crawler will search topic on the search page then crawl and gather information in the search results
- The process:
	1. Identify the topics of interest (by human)
	2. Automate searching topic and get search results (by machine)
	3. Get wanted contents from search results (by machine)
- Example (page 59 -> 61)
	- **Notice:** Better to loop through all topics and then through all websites -> evenly distribute and reduce the load placed on website at one moment.
##### Crawling Sites Through Links
- How? The crawler will crawl any link matching a specific URL pattern.
- Use cases:
	- Work well if you want to gather all the data from a site (not just from a specific search, search results are sometimes in poor quality)
	- Work well when the site's pages are disorganized and widely dispersed
- The process:
	1. Start in a page (normally the home page) -> parse wanted content (if exist) -> get internal links from page to the set
	2. Crawl a unvisited link -> parse wanted content (if exist) -> get unvisited internal links from page to the set
	3. Continue the process until there no more unvisited links.
- Example (page 62, 63)
##### Crawling Multiple Page Types
- Why we should identify page type?
	- Each page type holds different type of content that we want to scrap (or to skip)
	- Different page types have different layout -> need to be scrap differently -> specific code for each type.
- How to identify the type of the page we are crawling? Some basic ways:
	- By the URL: specific page type <-> specific URL pattern (e.g. a blog page: http://example.com/blog/title-of-post) 
	- By the presence or lack of certain fields on a site: e.g. if a page has a data but no author name, it can be a press release
	- By the presence of certain tags: e.g. `<div id="related-products">` to identify a product page
- How to keep track of multiple page type? Two ways (page 64, 65)
	1. Create a `page_type`  attribute in the Website class
	2. Create new subclass for each page type based on Webpage class (if new page types contain its own different attributes)
### [[Scrapy]]
### Storing Data
#### Storing Media Files
- Two ways to store media files:
	1. by reference (URL): for short-term used files
	2. by downloading: for long-term used files
- Using `urllib.request.urlretrieve` to download images from a remote URL:
	- e.g. ![[Pasted image 20220308091729.png|600]]
	- more complex example (page 85)
#### Storing CSV Files
- Using [`csv` library](https://docs.python.org/3/library/csv.html) in Python to read, write and manipulate csv file
	- Using `with open(filename, mode) as file_handler` to open a csv file, then:
		- `csv.reader()`
		- `csv.writer()`
	- Example (page 87)
#### Storing in MySQL
- Using *PyMySQL* library to [[Python#Intergrate SQL Database with Python|integrate Python with MySQL Database]]
- Example of scraping title and content of Wikipedia's pages and storing them to a MySQL Database using *BeautifulSoup* and *PyMySQL* libraries (page 96)
- Good practices for working with Database in Webscraping
	- Always add *id* columns as autoincremented and primary key (as there is no idea of unique scraped data in web scraping)
	- Use intelligent [[SQL Basics#Misc|indexing]]
	- Don't put the same (string) data in twice - use a relationship instead
	- Always close all `connection` and `cursor` objects
#### (Optional) Storing in Email

## Advanced Scraping
### Cleaning Dirty Data
### Crawling Through Forms and Logins 
#### `Requests` library
```ad-resources
[`Requests` library documentation](https://docs.python-requests.org/en/latest/)
```
- `Requests` library is an excellent HTTP handling library. It widely replace the built-in `urllib` module as the module's API is thoroughly broken.
#### Handling Logins and Cookies
### Crawling Through APIs 
### Web Crawling in Parallel
### The Legalities and Ethics


